{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHKiT Environmental Contours\n",
    "\n",
    "Environmental contours of extreme sea states can be used as a part of reliability-based design for offshore structures, including wave energy converters (WECs). Environmental contours provide estimations of extreme sea states based on short-term data (e.g. 10 years used to estimate a 100-year event). These environmental contours describe extreme sea states by characterizing the resource, defining sea states for extreme condition analysis, and developing a framework for analyzing survivability of a design.\n",
    "\n",
    "MHKiT includes functions adapted from the [WDRT](https://github.com/WEC-Sim/WDRT) for creating environmental contours of extreme sea states using a principal component analysis (PCA) methodology, with additional improvements for characterizing the joint probability distribution of sea states. As a demonstration, this notebook will walk through the following steps to find a 100-year sea state for NDBC buoy 46022 using 16 years of spectral wave density data.\n",
    "\n",
    " 1. Request Spectral Wave Density Data from NDBC\n",
    " 2. Calculate Hm0 and Te using the requested data\n",
    " 3. Find the data's 100-year contour\n",
    " 4. Plot the data and the 100-year contour\n",
    "\n",
    "We will start by importing the necessary python packages (`scipy`, `pandas`, `numpy`), and MHKiT `wave` submodules (`resource`, `contours`, `graphics`, and `io.ndbc`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhkit.wave import resource, contours, graphics\n",
    "import matplotlib.pyplot as plt\n",
    "from mhkit.wave.io import ndbc\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Request Spectral Wave Density Data from NDBC\n",
    "   \n",
    "MHKiT can be used to request historical data from the National Data Buoy Center ([NDBC](https://www.ndbc.noaa.gov/)). This process is split into the following steps:\n",
    "\n",
    "- Query available NDBC data  \n",
    "- Select years of interest \n",
    "- Request Data from NDBC\n",
    "- Convert the DataFrames to DateTime Index\n",
    " \n",
    "\n",
    "### Query available NDBC data  \n",
    "Looking at the help for the `ndbc.available_data` function (`help(ndbc.available_data)`) the function requires a parameter to be specified and optionally the user may provide a station ID as a string. A full list of available historical parameters can be found [here](https://www.ndbc.noaa.gov/data/historical/) although only some of these are currently supported.  We are interested in historical spectral wave density data `'swden'` (from which we may calculate Hm0 and Te). Additionally, we will specify the buoy number as `'46022'` to only return data associated with this site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the parameter as spectral wave density and the buoy number to be 46022\n",
    "parameter = 'swden'\n",
    "buoy_number = '46022' \n",
    "ndbc_available_data= ndbc.available_data(parameter, buoy_number)\n",
    "ndbc_available_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select years of interest\n",
    "\n",
    "The `ndbc.available_data` function has returned a DataFrame with columns 'id', 'year', and 'filename'. The year column is of type int while the filename and id (5 digit alpha-numeric specifier) are of type string. In this case, the years returned from `ndbc_available_data` span  1996 to the last complete year the buoy was operational (currently 2019 for 46022). For demonstration, we have decided we are interested in the data between the years 1996 and 2012 so we will create a new `years_of_interest` DataFrame which only contains years less than 2013.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Slice the available data to only include through year 2012\n",
    "years_of_interest = ndbc_available_data[ndbc_available_data.year < 2013]\n",
    "years_of_interest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Data from NDBC\n",
    "\n",
    "The filename column in our `years_of_interest` DataFrame and the parameter is needed to request the data. To get the data we can use the `ndbc.request_data` function to iterate over each buoy id and year in the passed DataFrame. This function will return the parameter data as a dictionary of DataFrames which may be accessed by buoy id and then the year for multiple buoys or just the year for a single buoy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary of parameter data by year\n",
    "filenames= years_of_interest['filename']\n",
    "ndbc_requested_data = ndbc.request_data(parameter, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the DataFrames to DateTime Index\n",
    "\n",
    "The data returned for each year has a variable number of columns for the year, month, day, hour, minute, and the way the columns are formatted (this is a primary reason for return a dictionary of DataFrames indexed by years). A common step a user will want to take is to remove the inconsistent NDBC date/ time columns and create a standard DateTime index. The MHKiT function `ndbc.to_datetime_index` will perform this standardization by parsing the NDBC date/ time columns into DateTime format and setting this as the DataFrame Index and removing the NDBC date/ time columns. This function operates on a DateFrame therefore we will iterate over each year of the `ndbc_requested_data` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we will convert a DateTime Index \n",
    "ndbc_data={}\n",
    "# Create a Datetime Index and remove NOAA date columns for each year\n",
    "for year in ndbc_requested_data:\n",
    "    year_data = ndbc_requested_data[year]\n",
    "    ndbc_data[year] = ndbc.to_datetime_index(parameter, year_data)\n",
    "\n",
    "# Display DataFrame of 46022 data from 1996\n",
    "ndbc_data['1996'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate $H_{m0}$ and $T_e$ using the NDBC Data\n",
    "\n",
    "A sea state may be characterized by significant wave height (Hm0) and energy period (Te). Using the historical spectral wave density data from NDBC, we can calculate these variables using MHKiT. Both Hm0 and Te return a single value for a given time (e.g. DateTime index). Currently, the data remains as a dictionary of DataFrames because the frequency binning (range and discretization) change across years of NDBC data. Once we have a single value for each DateTime we can combine all the data into a single DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize empty lists to store the results from each year\n",
    "Hm0_list=[]\n",
    "Te_list=[]\n",
    "\n",
    "# Iterate over each year and save the result in the initalized dictionary\n",
    "for year in ndbc_data:\n",
    "    year_data = ndbc_data[year]\n",
    "    Hm0_list.append(resource.significant_wave_height(year_data.T))\n",
    "    Te_list.append(resource.energy_period(year_data.T))\n",
    "\n",
    "# Concatenate list of Series into a single DataFrame\n",
    "Te = pd.concat(Te_list ,axis=0)\n",
    "Hm0 = pd.concat(Hm0_list ,axis=0)\n",
    "Hm0_Te = pd.concat([Hm0,Te],axis=1)\n",
    "\n",
    "# Drop any NaNs created from the calculation of Hm0 or Te\n",
    "Hm0_Te.dropna(inplace=True)\n",
    "# Sort the DateTime index\n",
    "Hm0_Te.sort_index(inplace=True)\n",
    "Hm0_Te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the contour line for the 100 year\n",
    "\n",
    "With the sea state data calculated, we can now use the modified I-FORM method to define reliability for a 100-year sea state based on the 17 years of spectral wave density data obtained from NDBC for buoy 46022. Reliability is the likelihood that a certain event will not occur in a given period. The period will define a line of constant probability in the joint probability of $H_{m0}$ and $T_e$ but individually each component different reliability (marginal distribution) which we can find by evaluating a normal cumulative distribution function (CDF). This CDF returns each component's quantiles along the iso-reliability line that finally allows us to calculate each sea state value (e.g. the 100-year contour values for Hm0 and Te). \n",
    "\n",
    "For more detail on the environmental contour method used here please refer to:\n",
    "[Eckert-Gallup et. al 2016](https://www.sciencedirect.com/science/article/abs/pii/S0029801815006721)\n",
    "\n",
    "To apply the `environmental_contours` function we will specify a 100-year sea state, the sea state data (Hm0, Te), and the time difference between measurements (dt in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return period (years) of interest\n",
    "period = 100  \n",
    "\n",
    "# Remove Hm0 Outliers\n",
    "Hm0_Te_clean = Hm0_Te[Hm0_Te.Hm0 < 20]\n",
    "\n",
    "# Get only the values from the DataFrame\n",
    "Hm0 = Hm0_Te_clean.Hm0.values  \n",
    "Te  = Hm0_Te_clean.Te.values \n",
    "\n",
    "# Delta time of sea-states \n",
    "dt = (Hm0_Te_clean.index[2]-Hm0_Te_clean.index[1]).seconds  \n",
    "\n",
    "# Get the contour values\n",
    "copula = contours.environmental_contours(Hm0, Te, dt, period, 'PCA', return_PCA=True)\n",
    "Hm0_contour=copula['PCA_x1']\n",
    "Te_contour=copula['PCA_x2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot overlay of the data and contour\n",
    "Lastly we can use the MHKiT graphics module to create a contour plot which shows the data and resultant conotour line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(8,4))\n",
    "#%matplotlib inline\n",
    "ax=graphics.plot_environmental_contour(Te, Hm0, \n",
    "                                      Te_contour, Hm0_contour, \n",
    "                                      data_label='NDBC 46022', \n",
    "                                      contour_label='100 Year Contour',\n",
    "                                      x_label = 'Energy Period, $Te$ [s]',\n",
    "                                      y_label = 'Sig. wave height, $Hm0$ [m]', \n",
    "                                      ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Contour Methods\n",
    "\n",
    "MHKiT has parametric, nonparametric, and Kernel Density Estimation methods for calculating environmental contours housed within the `resource.environmental_contours` function. We can compare other copulas to our PCA method simply by adding more methods. A single string method can be applied if only one copula is of interest as was done above for the PCA method or multiple methods can be sent in using a list of strings. If mutiple methods of the same type are desired is recommended to run them at the same time if possible as it will reduce the computational expense by utilizing the common computational calls across the copulas. In the example below we will compare the parametric and nonparametric Gaussain contours to the PCA method we ran previously. \n",
    "\n",
    "We start by using the default settings for our Gaussian methods. The `environmental_contours` function returns a dictionary with component 'x1' and 'x2' for each method. E.g. if the `environmental_contours` function were called with the `'gaussian'` method then `environmental_contours` function would return a dictionary with keys `['gaussian_x1', 'gaussian_x2']`. The copula methods are a generalized mathematical method and therefore 'x1' and 'x2' are used in place of Hm0 and Te for the component values. 'x1' refers to the first array passed and 'x2' refers to the second array passed. In the example below 'x1' would refer to the Hm0 component of the coupla and 'x2' would refer to Te. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copulas = contours.environmental_contours(Hm0, Te, dt, period, method=['gaussian', 'nonparametric_gaussian'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "\n",
    "Tes=[Te_contour]\n",
    "Hm0s=[Hm0_contour]\n",
    "methods=['gaussian', 'nonparametric_gaussian']\n",
    "for method in methods:   \n",
    "    Hm0s.append(copulas[f'{method}_x1'])\n",
    "    Tes.append(copulas[f'{method}_x2'])\n",
    "\n",
    "ax = graphics.plot_environmental_contour(Te, Hm0, \n",
    "                                         Tes, Hm0s,\n",
    "                                         data_label='NDBC 46050', \n",
    "                                         contour_label=['PCA','Gaussian', 'Nonparametric Gaussian'],\n",
    "                                         x_label = 'Energy Period, $Te$ [s]',\n",
    "                                         y_label = 'Sig. wave height, $Hm0$ [m]', \n",
    "                                         ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource Clusters\n",
    "\n",
    "Often in resource characterization we want to pick a few representative sea state to run an alaysis. To do this with the resource data in python we reccomend using a Gaussian Mixture Model (a more generalized k-means clustering method). Using sckitlearn this is very straigth forward. We combine our Hm0 and Te data into an N x 2 numpy array. We specify our number of components (number of representative sea states) and then call the fit method on the data. Fianlly, using the methods `means_` and `weights` we can organize the results into an easily digestable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize empty lists to store the results from each year\n",
    "Hm0_list=[]\n",
    "Tp_list=[]\n",
    "\n",
    "# Iterate over each year and save the result in the initalized dictionary\n",
    "for year in ndbc_data:\n",
    "    year_data = ndbc_data[year]  \n",
    "    Hm0_list.append(resource.significant_wave_height(year_data.T))\n",
    "    Tp_list.append(resource.peak_period(year_data.T))\n",
    "\n",
    "# Concatenate list of Series into a single DataFrame\n",
    "Tp = pd.concat(Tp_list ,axis=0)\n",
    "Hm0 = pd.concat(Hm0_list ,axis=0)\n",
    "Hm0_Tp = pd.concat([Hm0,Tp],axis=1)\n",
    "\n",
    "# Drop any NaNs created from the calculation of Hm0 or Te\n",
    "Hm0_Tp.dropna(inplace=True)\n",
    "# Sort the DateTime index\n",
    "Hm0_Tp.sort_index(inplace=True)\n",
    "\n",
    "# Remove Hm0 Outliers\n",
    "Hm0_Tp_clean = Hm0_Tp[Hm0_Tp.Hm0 < 20]\n",
    "Hm0_Tp_clean = Hm0_Tp[Hm0_Tp.Tp < 30]\n",
    "\n",
    "# Get only the values from the DataFrame\n",
    "Hm0 = Hm0_Tp_clean.Hm0.values  \n",
    "Tp  = Hm0_Tp_clean.Tp.values \n",
    "\n",
    "\n",
    "Hm0_Tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Compute Gaussian Mixture Model\n",
    "X = np.vstack((Tp, Hm0)).T\n",
    "gmm = GaussianMixture(n_components=8).fit(X)\n",
    "\n",
    "# Save centers and weights\n",
    "results = pd.DataFrame(gmm.means_, columns=['Tp','Hm0'])\n",
    "results['weights'] = gmm.weights_\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the Clusters\n",
    "\n",
    "We can visually look at the clusters by predicting which cluster each datapoint belongs in. Then we can plot the means on top of this to show where each cluster is centered and how the data points are being catagorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Sections of Data\n",
    "labels = gmm.predict(X)\n",
    "plt.scatter(Tp, Hm0, c=labels, s=40)\n",
    "plt.plot(results.Tp, results.Hm0, 'm+')\n",
    "plt.xlabel('Peak Period, $Tp$ [s]')\n",
    "plt.ylabel('Sig. wave height, $Hm0$ [m]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
